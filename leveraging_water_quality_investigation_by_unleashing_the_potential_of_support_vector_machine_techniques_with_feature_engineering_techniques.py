# -*- coding: utf-8 -*-
"""Leveraging water quality investigation by unleashing the potential of SUPPORT VECTOR MACHINE Techniques with feature engineering techniques.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BLYba9vpYKrEZSftgi1Dz5BdXS102rj4

# **Leveraging water quality investigation by unleashing the potential of SUPPORT VECTOR MACHINE Techniques with feature engineering techniques**



## 1.   Data Loading and Preliminary Analysis:

*   Load the dataset into memory.
*  
Perform a preliminary analysis to understand the data structure and distributions

## 2.   Feature Engineering and Selection:

*  
Apply ANOVA, Kruskal-Wallis Test, MRMR (Minimum Redundancy Maximum Relevance)
and Chi-Square Test for feature selection to identify the most relevant features for the SVM model.


## 3.   SVM Model Training:


*   Train different types of SVM models (e.g., linear, polynomial, radial basis function) on the selected features.
*   Optimize the SVM models using cross-validation and grid search for hyperparameter tuning.


## 4.   Model Evaluation:


*  Evaluate the performance of the optimized SVM models using appropriate metrics (e.g., accuracy, precision, recall, F1-score).


## 5.   Web Application Development:


*   Develop a simple web application that allows users to input water quality parameters.
*
Connect the SVM model to the web application to predict whether the water is safe to drink based on user input.



## 6.   Integration and Testing:


*   Integrate the SVM model with the web application
*   
Test the end-to-end functionality of the web application.


## 7.   Deployment


*  Prepare the web application and the SVM model for deployment.
*   Deploy the web application to a web server.

# **Data Analysis**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

df = pd.read_csv("waterQuality21.csv")
df.head()

df.info()

df.describe

df.isna().sum(axis=0)

temp = df.describe()
temp.style.background_gradient(cmap='Oranges')

# Convert non-numeric columns to numeric (if possible)
for col in df.columns:
    if df[col].dtype == 'object':
        try:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        except ValueError:
            print(f"Column '{col}' cannot be converted to numeric.")
    # Drop rows with NaN values in the specified column
    df.dropna(subset=['ammonia'], inplace=True)

# Check data types after conversion
print(df.dtypes)

# Histogram
# Define the number of rows and columns for the subplot grid
num_rows = 4
num_cols = (len(df.columns) + num_rows - 1) // num_rows  # Calculate the number of columns dynamically

plt.figure(figsize=(15, 8))

for i, col in enumerate(df.columns, 1):
    plt.subplot(num_rows, num_cols, i)
    sns.histplot(df[col].dropna(), kde=True)
    plt.title(f'{col} Distribution')

plt.tight_layout()
plt.show()

#Box Plot
plt.figure(figsize=(15, 8))

for i, col in enumerate(df.columns, 1):
    plt.subplot(num_rows, num_cols, i)
    sns.boxplot(y=df[col].dropna())
    plt.title(f'{col} Levels')

plt.tight_layout()
plt.show()

"""# **Data Preprocessing**"""

from sklearn.model_selection import train_test_split

X = df.drop(columns=['is_safe'],axis=1)  # Features
y = df['is_safe']  # Target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)

scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train

"""# **Most Relevant features Using Annova**
## The ANOVA F-test has identified the top 12 features that explain the most variance in relation to the target variable 'is_safe'. These features are, in order of importance: aluminium, cadmium, chloramine, chromium, arsenic, silver, viruses, barium, perchlorate, uranium,nitrates and radium. These features are likely to be influential in predicting water safety and will be considered for the SVM model.
---


"""

from sklearn.feature_selection import SelectKBest, f_classif

# Define the features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Apply SelectKBest class to extract top 10 features using ANOVA F-test
bestfeatures = SelectKBest(score_func=f_classif, k=10)
fit = bestfeatures.fit(X,y)
df_scores = pd.DataFrame(fit.scores_)
df_columns = pd.DataFrame(X.columns)

# Concat two dataframes for better visualization
feature_scores = pd.concat([df_columns,df_scores],axis=1)
feature_scores.columns = ['Feature','Score']  # Naming the dataframe columns

# Print the 10 best features selected by ANOVA F-test
print('Top 12 features selected by ANOVA F-test:')
print(feature_scores.nlargest(12,'Score'))

"""# **Most Relevant features Using Kruskal-Wallis Test and their P-value**
## Krushkal-Wallis  test is non-parametric and does not assume a normal distribution of the data, which can be useful if some features are not normally distributed.The Kruskal-Wallis Test will identify the top 12 features with the lowest p-values. These features are likely to have statistically significant differences between the groups defined by the target variable 'is_safe'.
"""

from scipy.stats import kruskal

# Perform Kruskal-Wallis H-test
# This is a non-parametric method used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable
kruskal_results = {}
for column in X.columns:
    groups = X[column].groupby(y).apply(list)
    kruskal_results[column] = kruskal(*groups)[1]  # p-value

# Convert the results to a dataframe
kruskal_df = pd.DataFrame(list(kruskal_results.items()), columns=['Feature', 'P-Value'])

# Sort the dataframe by the p-value in ascending order to see the most significant features
kruskal_df = kruskal_df.sort_values('P-Value')

# Display the top 10 features with the lowest p-values
print('Top 12 features selected by Kruskal-Wallis Test:')
print(kruskal_df.head(12))

"""# **Most Relevant Features using MRMR**
## The MRMR feature selection will identify the top 12 features with the highest mutual information value. These features have a strong mutual dependence with the target variable 'is_safe'.The MRMR feature selection will  provide us with a list of features that have a strong mutual dependence with the target variable 'is_safe', which could be valuable for the SVM model.
"""

from sklearn.feature_selection import mutual_info_classif

# Apply the mutual_info_classif to get mutual information
mi = mutual_info_classif(X, y)
mi_df = pd.DataFrame(mi, index=X.columns, columns=['Mutual Information'])

# Sort the dataframe by mutual information in descending order
mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)

# Display the top 10 features with the highest mutual information
print('Top 12 features selected by MRMR:')
print(mi_df.head(12))

"""# **Chi-Square test, to evaluate the independence of the features from the target variable**
## The Chi-Square test will identify the top 12 features with the highest Chi-Square values. These features are likely to be the most independent from the target variable 'is_safe' and could be important for the SVM model.
"""

from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler

# Assuming X is your feature matrix and y is your target variable
# If X contains negative values, scale it to a non-negative range
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Apply the chi2 function to get chi-squared stats
chi_scores = chi2(X_scaled, y)
chi2_df = pd.DataFrame(chi_scores, index=['Chi-Square', 'P-Value'], columns=X.columns).transpose()

# Sort the dataframe by the chi-squared stats in descending order
chi2_df = chi2_df.sort_values(by='Chi-Square', ascending=False)

# Display the top 10 features with the highest chi-squared stats
print('Top 12 features selected by Chi-Square test:')
print(chi2_df.head(12))

"""# **Linear SVM**"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

# Train the SVM model with a linear kernel
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the accuracy and the classification report
print('Accuracy:', accuracy*100)
print(report)

"""# **Linear SVM with chi square**"""

from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import MinMaxScaler

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Scale the features to be non-negative as required by Chi-Square
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Apply Chi-Square feature selection
chi2_selector = SelectKBest(chi2, k='all') # Let's select all features for now to see the scores
X_kbest_features = chi2_selector.fit_transform(X_scaled, y)

# Get the scores of the features
chi2_scores = chi2_selector.scores_

# Create a dataframe to display feature scores
feature_scores_df = pd.DataFrame({'Feature': X.columns, 'Chi2Score': chi2_scores})
feature_scores_df = feature_scores_df.sort_values(by='Chi2Score', ascending=False)

# print(feature_scores_df)

# For the SVM, we will select the top N features. Let's choose the top 10 for now.
# This number can be tuned later.
top_features = feature_scores_df['Feature'].iloc[:12].tolist()
print(top_features)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X[top_features], y, test_size=0.2, random_state=42)

# Train the SVM model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print('Accuracy:', accuracy*100)
print(report)

"""# **Linear SVM with MRMR**"""

!pip install skfeature-chappers

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import skfeature.function.information_theoretical_based.MRMR as mrmr

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using MRMR
selected_features = mrmr.mrmr(X_standardized, y, n_selected_features=10)

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_standardized[:, selected_features], y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', X.columns[selected_features].tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Linear SVM with ANNOVA**"""

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using ANOVA F-test
selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X_standardized, y)

# Get the indices of the selected features
selected_indices = selector.get_support(indices=True)
selected_feature_names = X.columns[selected_indices]

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_feature_names.tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Linear SVM with Krushal Wallis**"""

from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import kruskal
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using Kruskal-Wallis test
# Calculate the Kruskal-Wallis H-test for each feature
scores, pvalues = [], []
for i in range(X_standardized.shape[1]):
    score, pvalue = kruskal(X_standardized[y==0, i], X_standardized[y==1, i])
    scores.append(score)
    pvalues.append(pvalue)

# Select the 10 features with the highest score
selected_indices = np.argsort(scores)[-10:]
selected_feature_names = X.columns[selected_indices]

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_standardized[:, selected_indices], y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_feature_names.tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Cubic SVM**"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Train the SVM model with a cubic kernel
X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='poly', degree=3)
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the accuracy and the classification report
print('Accuracy:', accuracy*100)
print(report)

"""# **Cubic SVM with Chi square **"""

from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X_scaled)

# Feature selection using Chi-Square test
selector = SelectKBest(chi2, k=10)
X_selected = selector.fit_transform(X_scaled, y)  # Note: Chi-Square requires non-negative features

# Get the indices of the selected features
selected_indices = selector.get_support(indices=True)
selected_feature_names = X.columns[selected_indices]

# Train the SVM model with a cubic kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='poly', degree=3)
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_feature_names.tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Cubic SVM with MRMR**"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import skfeature.function.information_theoretical_based.MRMR as mrmr

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using MRMR
selected_features = mrmr.mrmr(X_standardized, y, n_selected_features=10)

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_standardized[:, selected_features], y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='poly', degree=3)
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', X.columns[selected_features].tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Cubic SVM with Krushkal Wallis**

"""

from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import kruskal
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using Kruskal-Wallis test
# Calculate the Kruskal-Wallis H-test for each feature
scores, pvalues = [], []
for i in range(X_standardized.shape[1]):
    score, pvalue = kruskal(X_standardized[y==0, i], X_standardized[y==1, i])
    scores.append(score)
    pvalues.append(pvalue)

# Select the 10 features with the highest score
selected_indices = np.argsort(scores)[-10:]
selected_feature_names = X.columns[selected_indices]

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_standardized[:, selected_indices], y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='poly', degree=3)
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_feature_names.tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Cubic SVM with Annova**"""

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using ANOVA F-test
selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X_standardized, y)

# Get the indices of the selected features
selected_indices = selector.get_support(indices=True)
selected_feature_names = X.columns[selected_indices]

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='poly', degree=3)
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_feature_names.tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Fine Gaussian SVM**"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif
import pandas as pd

# Load the dataset
file_path = 'waterQuality21.csv'
df = pd.read_csv(file_path)

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)



# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Gaussian SVM model
svm_model = SVC(kernel='rbf')
svm_model.fit(X_train, y_train)

# Make predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_features)
print('Accuracy:', accuracy*100)
print(report)

"""# **Fine Gaussian SVM with chi sqaure**"""

from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import MinMaxScaler

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Scale the features to be non-negative as required by Chi-Square
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Apply Chi-Square feature selection
chi2_selector = SelectKBest(chi2, k='all') # Let's select all features for now to see the scores
X_kbest_features = chi2_selector.fit_transform(X_scaled, y)

# Get the scores of the features
chi2_scores = chi2_selector.scores_

# Create a dataframe to display feature scores
feature_scores_df = pd.DataFrame({'Feature': X.columns, 'Chi2Score': chi2_scores})
feature_scores_df = feature_scores_df.sort_values(by='Chi2Score', ascending=False)

# print(feature_scores_df)

# For the SVM, we will select the top N features. Let's choose the top 10 for now.
# This number can be tuned later.
top_features = feature_scores_df['Feature'].iloc[:12].tolist()
print(top_features)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X[top_features], y, test_size=0.2, random_state=42)

# Train the SVM model
svm_model = SVC(kernel='rbf')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print('Accuracy:', accuracy*100)
print(report)

"""# **Fine Gaussain with MRMR**"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import skfeature.function.information_theoretical_based.MRMR as mrmr

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using MRMR
selected_features = mrmr.mrmr(X_standardized, y, n_selected_features=12)

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_standardized[:, selected_features], y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='rbf')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', X.columns[selected_features].tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Fine Gaussian with Annova**"""

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using ANOVA F-test
selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X_standardized, y)

# Get the indices of the selected features
selected_indices = selector.get_support(indices=True)
selected_feature_names = X.columns[selected_indices]

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='rbf')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_feature_names.tolist())
print('Accuracy:', accuracy*100)
print(report)

"""# **Fine Gaussian with Krushkal Wallis**"""

from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import kruskal
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Feature selection using Kruskal-Wallis test
# Calculate the Kruskal-Wallis H-test for each feature
scores, pvalues = [], []
for i in range(X_standardized.shape[1]):
    score, pvalue = kruskal(X_standardized[y==0, i], X_standardized[y==1, i])
    scores.append(score)
    pvalues.append(pvalue)

# Select the 10 features with the highest score
selected_indices = np.argsort(scores)[-10:]
selected_feature_names = X.columns[selected_indices]

# Train the SVM model with a linear kernel using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_standardized[:, selected_indices], y, test_size=0.2, random_state=42)
svm_model = SVC(kernel='rbf')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output the selected features, the accuracy and the classification report
print('Selected Features:', selected_feature_names.tolist())
print('Accuracy:', accuracy*100)
print(report)

import matplotlib.pyplot as plt

# Visualize the selected features
selected_features = ['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
df[selected_features].plot(kind='box', vert=False, figsize=(12,8))
plt.title('Boxplot of Selected Water Quality Features')
plt.xlabel('Concentration')
plt.show()

# Calculate the safe range for the selected features
safe_range = df[selected_features].agg(['min', 'max'])
safe_range

!pip install skfeature-chappers

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
import joblib
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np

# Assuming mrmr is already imported and configured

# Load the dataset
water_quality_df = pd.read_csv('waterQuality21.csv', encoding='ascii')

X = water_quality_df.drop('is_safe', axis=1)
y = water_quality_df['is_safe']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_standardized = scaler.fit_transform(X_train)
X_test_standardized = scaler.transform(X_test)

# Feature selection with MRMR
selected_features_indices = mrmr.mrmr(X_train_standardized, y_train, n_selected_features=12)

# Custom transformer for feature selection
class FeatureSelector(BaseEstimator, TransformerMixin):
    def __init__(self, feature_indices):
        self.feature_indices = feature_indices

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[:, self.feature_indices]

# Create the pipeline
pipeline = make_pipeline(
    StandardScaler(),
    FeatureSelector(feature_indices=selected_features_indices),
    MinMaxScaler(),
    SVC(kernel='rbf', gamma='auto')
)

# Fit the pipeline
pipeline.fit(X_train, y_train)

# Save the pipeline
pipeline_filename = 'svm_model_features.joblib'
joblib.dump(pipeline, pipeline_filename)

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib

# Load the dataset
file_path = 'waterQuality21.csv'
data = pd.read_csv(file_path)

# Define the feature set based on the top features from MRMR
selected_features = ['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate',
                     'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
X = data[selected_features]
y = data['is_safe']

# Standardize the features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

# Load the saved model
model_filename = 'svm_model_12_features.joblib'
svm_model = joblib.load(model_filename)

# Define the function to predict water safety
example_data = [0.0171,1.8443,0.0747,0.3843,3.5168,20.1795,1.4052,3.3354,0.2267,0.1888,0.039,1.8748]
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.14,6.06,2.05,1.09,9.89,61.89,3.986,8.567,1.0,0.667,0.09,5.96]
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.0012,4.0005,0.0001,0.0009,7.6667,27.745,1.0093,5.4445,0.000010,0.0012,0.000001,3.994]
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.0023,4.567,0.0054,0.089,7.347,24.577,1.983,5.6789,0.0012,0.0032,0.007,3.234]
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [12.03,6.06,4.05,2.09,10.89,24.89,4.986,9.567,1.0,0.667,0.09,5.96]
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.005,2.9,0.02,1.21,5.09,57.89,2.34,6.76,0.5,0.21,1.03,3.99]#['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.0012,1.0001,0.00012,0.00091,1.113,10.223,2.34,1.345,0.0001,0.00012,0.00211,1.1221]#['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.0022,2.0001,0.00012,0.2091,1.113,30.223,1.34,2.345,0.0001,0.00012,0.00211,4.1221]#['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])  # Nitrites the deciding value safety feature
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.012,2.51,0.0012,0.0091,1.113,50.223,2.1,2.345,0.0001,0.00012,0.00211,4.1221]#['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])  # Nitrates the deciding value safety feature
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [1,1,1,1,1,1,1,1,1,1,1,1]#['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [0.13,1.23,2.34,5.67,10.96,20.35,1.2,0.8,4.12,3.21,1.52,0.12]#['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result

example_data = [-1,0,-1,0,-1,-1,-1,-1,-1,-1,-1,-1]#['cadmium', 'aluminium', 'arsenic', 'chromium', 'chloramine', 'perchlorate', 'nitrites', 'radium', 'viruses', 'silver', 'uranium', 'barium']
def predict_water_safety(sample_data):
    sample_data_standardized = scaler.transform([sample_data])
    prediction = svm_model.predict(sample_data_standardized)
    return 'Safe' if prediction == 1 else 'Not Safe'

# Predict the safety of the example data
prediction_result = predict_water_safety(example_data)
prediction_result